<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>A Recipe for Causal Graph Regression - Paper Structure</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            padding: 40px 20px;
            min-height: 100vh;
        }
        
        .container {
            max-width: 900px;
            margin: 0 auto;
            background-color: white;
            border-radius: 10px;
            box-shadow: 0 10px 40px rgba(0, 0, 0, 0.2);
            overflow: hidden;
        }
        
        .header {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 40px 30px;
            text-align: center;
        }
        
        .header h1 {
            font-size: 2em;
            margin-bottom: 10px;
            font-weight: 700;
        }
        
        .header p {
            font-size: 1.1em;
            opacity: 0.95;
            font-style: italic;
        }
        
        .content {
            padding: 40px 30px;
        }
        
        .section {
            margin-bottom: 30px;
        }
        
        .section h2 {
            color: #667eea;
            font-size: 1.6em;
            margin-bottom: 15px;
            padding-bottom: 10px;
            border-left: 4px solid #667eea;
            padding-left: 15px;
        }
        
        .subsection {
            margin-left: 20px;
            margin-bottom: 12px;
            padding: 10px 15px;
            background-color: #f8f9fa;
            border-left: 3px solid #764ba2;
            border-radius: 4px;
        }
        
        .subsection h3 {
            color: #764ba2;
            font-size: 1.2em;
            font-weight: 600;
            margin-bottom: 5px;
        }
        
        .subsection ul {
            list-style-position: inside;
            color: #555;
            font-size: 0.95em;
            line-height: 1.6;
        }
        
        .subsection ul li {
            margin-left: 15px;
            margin-bottom: 5px;
        }
        
        .toc {
            background-color: #f0f4ff;
            padding: 25px;
            border-radius: 8px;
            margin-bottom: 30px;
        }
        
        .toc h2 {
            color: #667eea;
            font-size: 1.4em;
            margin-bottom: 15px;
            padding-bottom: 10px;
            border-left: 4px solid #667eea;
            padding-left: 15px;
        }
        
        .toc ol {
            color: #555;
            line-height: 1.8;
            padding-left: 20px;
        }
        
        .toc li {
            margin-bottom: 8px;
        }
        
        .key-info {
            background-color: #fff3cd;
            border-left: 4px solid #ffc107;
            padding: 15px;
            margin: 20px 0;
            border-radius: 4px;
        }
        
        .key-info strong {
            color: #856404;
        }
        
        .footer {
            background-color: #f8f9fa;
            padding: 20px 30px;
            text-align: center;
            color: #666;
            font-size: 0.9em;
            border-top: 1px solid #ddd;
        }
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>A Recipe for Causal Graph Regression</h1>
            <p>Confounding Effects Revisited</p>
        </div>
        
        <div class="content">
            <div class="key-info">
                <strong>Paper Source:</strong> ICML 2025 - Authors: Yujia Yin, Tianyi Qu, Zihao Wang, Yifan Chen
            </div>
            
            <div class="toc">
                <h2>ðŸ“‹ Paper Structure Overview</h2>
                <ol>
                    <li><strong>Introduction</strong> - Addressing Causal Graph Regression</li>
                    <li><strong>Related Work</strong> - OOD Challenges and Existing Approaches</li>
                    <li><strong>Preliminaries and Notations</strong> - Fundamental Concepts</li>
                    <li><strong>Revisiting Confounding Effects for CGR</strong> - Core Methodology</li>
                    <li><strong>Experiments</strong> - Evaluation and Results</li>
                    <li><strong>Conclusion</strong> - Summary and Impact</li>
                </ol>
            </div>
            
            <div class="section">
                <h2>1. Introduction</h2>
                <div class="subsection">
                    <h3>Main Topic</h3>
                    <ul>
                        <li>Causal Graph Learning (CGL) in Classification vs. Regression</li>
                        <li>Challenges of extending CGL to regression tasks</li>
                        <li>Importance of understanding confounding effects in graph regression</li>
                    </ul>
                </div>
            </div>
            
            <div class="section">
                <h2>2. Related Work</h2>
                <div class="subsection">
                    <h3>Out-of-Distribution (OOD) Challenges in Graph Learning</h3>
                    <ul>
                        <li>Approaches categorized into: Invariant Learning, Causal Modeling, Stable Learning</li>
                    </ul>
                </div>
                <div class="subsection">
                    <h3>Invariant Learning</h3>
                    <ul>
                        <li>Identifying features stable across environments</li>
                        <li>Methods: CIGA, GSAT, GALA</li>
                    </ul>
                </div>
                <div class="subsection">
                    <h3>Causal Modeling</h3>
                    <ul>
                        <li>Leveraging Structural Causal Models (SCMs)</li>
                        <li>Techniques: Backdoor adjustment, Frontdoor adjustment, Instrumental variables</li>
                        <li>Limitations in regression settings</li>
                    </ul>
                </div>
                <div class="subsection">
                    <h3>Stable Learning</h3>
                    <ul>
                        <li>Consistent performance across environments</li>
                        <li>Sample reweighting and covariate distribution balancing</li>
                    </ul>
                </div>
            </div>
            
            <div class="section">
                <h2>3. Preliminaries and Notations</h2>
                <div class="subsection">
                    <h3>Causal Graph Learning</h3>
                    <ul>
                        <li>Graph decomposition: G = C âŠ• S (Causal + Confounding subgraphs)</li>
                        <li>Learnable soft masks for graph filtering</li>
                        <li>Mutual Information computation</li>
                    </ul>
                </div>
                <div class="subsection">
                    <h3>Graph Information Bottleneck (GIB)</h3>
                    <ul>
                        <li>Balancing information preservation and compression</li>
                        <li>Minimizing I(C; G) while maximizing I(C; Y)</li>
                        <li>Identifying causally relevant subgraphs</li>
                    </ul>
                </div>
                <div class="subsection">
                    <h3>Causal Intervention in GNNs</h3>
                    <ul>
                        <li>Structural Causal Model (SCM) representation</li>
                        <li>Backdoor adjustment principle</li>
                        <li>Addressing spurious correlations (S)</li>
                    </ul>
                </div>
            </div>
            
            <div class="section">
                <h2>4. Revisiting Confounding Effects for CGR</h2>
                <div class="subsection">
                    <h3>Overview</h3>
                    <ul>
                        <li>Framework combining Enhanced GIB with Causal Discovery</li>
                        <li>GNN-based encoder for graph embeddings</li>
                        <li>Attention modules for soft mask generation</li>
                        <li>Contrastive Learning integration</li>
                    </ul>
                </div>
                <div class="subsection">
                    <h3>Enhanced GIB Objective</h3>
                    <ul>
                        <li>Reconsidering predictive power of confounding subgraphs</li>
                        <li>Extended GIB loss: -I(C; Y) + Î±I(C; G) - Î²I(S; Y)</li>
                        <li>Variational bounds for I(C; G), I(C; Y), and I(S; Y)</li>
                        <li>Practical computation using Gaussian distributions</li>
                    </ul>
                </div>
                <div class="subsection">
                    <h3>Causal Intervention</h3>
                    <ul>
                        <li>Random addition method for counterfactual graphs</li>
                        <li>Contrastive learning-based causal intervention loss</li>
                        <li>Alignment of original and mixed graph representations</li>
                        <li>InfoNCE loss formulation</li>
                    </ul>
                </div>
            </div>
            
            <div class="section">
                <h2>5. Experiments</h2>
                <div class="subsection">
                    <h3>Datasets</h3>
                    <ul>
                        <li><strong>GOOD-ZINC:</strong> Molecular property regression with 4 OOD types</li>
                        <li><strong>ReactionOOD-SOOD:</strong> Three S-OOD datasets (Cycloaddition, E2&SN2, RDB7)</li>
                    </ul>
                </div>
                <div class="subsection">
                    <h3>Baselines and Setup</h3>
                    <ul>
                        <li>Euclidean OOD methods: IRM, VREx, GroupDRO, DANN, CORAL, Mixup</li>
                        <li>Graph OOD methods: CIGA, GSAT, DIR</li>
                        <li>Consistent architectures across all methods</li>
                        <li>Three repetitions with different random seeds</li>
                    </ul>
                </div>
                <div class="subsection">
                    <h3>Results of GOOD</h3>
                    <ul>
                        <li>State-of-the-art (SOTA) performance on GOOD-ZINC</li>
                        <li>42.2% improvement over GSAT in Scaffold Covariate (ID)</li>
                        <li>Significant improvements across both ID and OOD settings</li>
                        <li>Reduced variance indicating stability</li>
                    </ul>
                </div>
                <div class="subsection">
                    <h3>Results of ReactionOOD</h3>
                    <ul>
                        <li>Best OOD performance in 6 out of 10 cases</li>
                        <li>Second-best in 2 cases with minimal performance gaps</li>
                        <li>Robust generalization across multiple datasets</li>
                        <li>Consistent outperformance of causal methods like CIGA</li>
                    </ul>
                </div>
                <div class="subsection">
                    <h3>Effectiveness in Classification Task</h3>
                    <ul>
                        <li>Ablation studies on GOOD-Motif dataset</li>
                        <li>Validation of confounding predictive power assumption</li>
                        <li>Effectiveness of contrastive learning approach</li>
                    </ul>
                </div>
            </div>
            
            <div class="section">
                <h2>6. Conclusion</h2>
                <div class="subsection">
                    <h3>Key Contributions</h3>
                    <ul>
                        <li>First explicit consideration of predictive role of confounding features</li>
                        <li>New causal intervention approach using contrastive learning</li>
                        <li>Improved OOD generalization for graph regression tasks</li>
                        <li>Enhanced understanding of confounding effects in CGL</li>
                    </ul>
                </div>
            </div>
            
            <div class="key-info">
                <strong>ðŸ“Š Key Innovation:</strong> Extended GIB loss that considers the predictive capacity of both causal and confounding subgraphs, combined with contrastive learning for robust causal intervention without relying on discrete label information.
            </div>
        </div>
        
        <div class="footer">
            <p>Generated from: "A Recipe for Causal Graph Regression: Confounding Effects Revisited"</p>
            <p>ICML 2025 | <a href="https://github.com/causal-graph/CGR" style="color: #667eea; text-decoration: none;">GitHub Repository</a></p>
        </div>
    </div>
</body>
</html>
